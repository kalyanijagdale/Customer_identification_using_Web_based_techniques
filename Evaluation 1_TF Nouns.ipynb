{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_clean_eBay_tweets_drop2.csv\n",
      "pos_clean_ladygaga_tweets_drop2.csv\n",
      "pos_clean_premierleague_tweets_drop2.csv\n",
      "pos_clean_parenting_tweets.csv\n",
      "pos_clean_FoodandTravelEd_tweets_drop2.csv\n",
      "pos_clean_usedgov_tweets.csv\n",
      "pos_clean_facebook_tweets.csv\n",
      "pos_clean_nytimes_tweets.csv\n",
      "pos_clean_tesla_tweets_drop2.csv\n",
      "pos_clean_eBay_tweets.csv\n",
      "pos_clean_tesla_tweets.csv\n",
      "pos_clean_facebook_tweets_drop2.csv\n",
      "pos_clean_MTV_tweets.csv\n",
      "pos_clean_FoodandTravelEd_tweets.csv\n",
      "pos_clean_premierleague_tweets.csv\n",
      "pos_clean_MTV_tweets_drop2.csv\n",
      "pos_clean_parenting_tweets_drop2.csv\n",
      "pos_clean_nytimes_tweets_drop2.csv\n",
      "pos_clean_ladygaga_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "goal_dir = os.path.join(os.getcwd(), \"pos_csv/\")\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(goal_dir):\n",
    "    if filename.endswith(\".csv\"): \n",
    "         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC MODELLING RESULTS\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eBay_tweets_drop2.csv done\n",
      "ladygaga_tweets_drop2.csv done\n",
      "premierleague_tweets_drop2.csv done\n",
      "parenting_tweets.csv done\n",
      "FoodandTravelEd_tweets_drop2.csv done\n",
      "usedgov_tweets.csv done\n",
      "facebook_tweets.csv done\n",
      "nytimes_tweets.csv done\n",
      "tesla_tweets_drop2.csv done\n",
      "eBay_tweets.csv done\n",
      "tesla_tweets.csv done\n",
      "facebook_tweets_drop2.csv done\n",
      "MTV_tweets.csv done\n",
      "FoodandTravelEd_tweets.csv done\n",
      "premierleague_tweets.csv done\n",
      "MTV_tweets_drop2.csv done\n",
      "parenting_tweets_drop2.csv done\n",
      "nytimes_tweets_drop2.csv done\n",
      "ladygaga_tweets.csv done\n"
     ]
    }
   ],
   "source": [
    "#TERM FREQUENCY COUNTS\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "goal_dir = os.path.join(os.getcwd(), \"pos_csv/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(goal_dir):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        file = pd.read_csv(\"pos_csv/\"+filename)\n",
    "        file=file[file['cleaned_word_list_nouns'].notnull()]\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        cvmatrix = vectorizer.fit_transform(file['cleaned_word_list_nouns'])\n",
    "\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "        cv_data=cvmatrix.toarray().sum(axis=0)\n",
    "        \n",
    "        #print(type(vocab))\n",
    "        #print((cv_data))\n",
    "        cv_pd=pd.DataFrame(list(zip(vocab, cv_data)), columns =['Term', 'frequency']) \n",
    "        cv_pd= cv_pd.sort_values(by='frequency', ascending=False)\n",
    "        #print(cv_pd.head())\n",
    "    \n",
    "        \n",
    "        \n",
    "        cv_pd.to_csv(r'terms_frequency_nouns/'+filename[10:], encoding='utf-8')\n",
    "        \n",
    "        number_of_topics = 10\n",
    "        model = LatentDirichletAllocation(n_components=number_of_topics, random_state=45) # random state for reproducibility\n",
    "        # Fit data to model\n",
    "        model.fit(cvmatrix)\n",
    "        df= display_topics(model,vocab,10)\n",
    "        \n",
    "        df.to_csv(r'topic_modelling_nouns/'+filename[10:], encoding='utf-8')\n",
    "        \n",
    "        print(filename[10:],\"done\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
